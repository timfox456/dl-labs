{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression With Tensorflow (House Prices)\n",
    "Let's predict some house prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = '../data/house-prices/house-sales-full.csv'\n",
    "# data_location = 'https://elephantscale-public.s3.amazonaws.com/data/house-prices/house-sales-full.csv'\n",
    "\n",
    "house_prices = pd.read_csv(data_location)\n",
    "house_prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Cleanup Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"original row count : \", house_prices.shape)\n",
    "house_prices = house_prices.dropna()\n",
    "print (\"cleaned up row count : \", house_prices.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 : Exploratory Data Analysis (EDA)\n",
    "EDA will give us a sense of data.  It is highly recommended that you do this before learning.\n",
    "\n",
    "**==> Q : What is max number of bedrooms? :-)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get a summary of data\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "## TODO : use 'describe()' function to get summary info\n",
    "house_prices.???().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Remove Outliers\n",
    "As you can see we have a few outliers.  \n",
    "Let's remove them by considering only houses with less than 5 BR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : commented out for now, \n",
    "##        uncomment during tuning phase\n",
    "\n",
    "# house_prices = house_prices[house_prices['Bedrooms'] <= 5]\n",
    "# house_prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 : Choose Columns to consider\n",
    "Which attributes do you think are important in deciding SalePrice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : Experiment with this, \n",
    "## select columns you think are important in determining SalePrice\n",
    "## Hint : Start with : 'Bedrooms', 'Bathrooms', 'SqFtTotLiving', 'SqFtLot'\n",
    "input_columns = ['???', '???', '???', '???']\n",
    "label_column = 'SalePrice'\n",
    "# x = house_prices.loc[:, input_columns]\n",
    "x = house_prices [input_columns]\n",
    "y = house_prices[[label_column]]\n",
    "\n",
    "print(x.head())\n",
    "print ('--------')\n",
    "print (y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 :  Split data into train /test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## TODO split train/test = 80% / 20%\n",
    "## Hint : test_size=0.2  (representing 20%)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = ???, random_state = 0)\n",
    "\n",
    "x_train_orig = x_train\n",
    "x_test_orig = x_test\n",
    "\n",
    "print (\"x_train.shape : \", x_train.shape)\n",
    "print (\"y_train.shape : \", y_train.shape)\n",
    "print (\"x_test.shape : \", x_test.shape)\n",
    "print (\"y_test.shape : \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 : Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To turn off scaling, comment this cell out\n",
    "\n",
    "def my_scaler(df):\n",
    "    #return (df-df.min())/(df.max()-df.min())  ## this is min/max scaler\n",
    "    return (df - df.mean()) / df.std()\n",
    "\n",
    "print (\"x_train: before and after\")\n",
    "print(x_train_orig.head())\n",
    "x_train = my_scaler(x_train_orig)\n",
    "print(x_train.head())\n",
    "\n",
    "print ('-----')\n",
    "print ('x_test: before / after')\n",
    "print (x_test_orig.head())\n",
    "x_test = my_scaler (x_test_orig)\n",
    "print (x_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8:  Build a Model\n",
    "\n",
    "Build a 3 layer network\n",
    "- input (64 neurons)\n",
    "- hidden (64 neurons)\n",
    "- output (1 neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    input_dim = len(x_train.keys())\n",
    "    print (\"input_dim : \", input_dim)\n",
    "    ## TODO : define a a model\n",
    "    ##   - add 64 neurons (units=64) for 'input_layer'  and 'hidden_1' layer\n",
    "    ##   - final outupt layer has ONE neuron  (units=1)\n",
    "    model = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(units=???, activation=tf.nn.relu, input_shape=[input_dim], name=\"input_layer\"),\n",
    "                tf.keras.layers.Dense(units=???, activation=tf.nn.relu, name=\"hidden_1\"),\n",
    "                tf.keras.layers.Dense(units=???, name=\"output_layer\")\n",
    "            ])\n",
    "\n",
    "    ## TODO : We start with RMSProp.  Feel free to try other optimizers\n",
    "    optimizer = tf.keras.optimizers.RMSprop(0.01)\n",
    "\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['mean_absolute_error', 'mean_squared_error'])\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Setup Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is fairly boiler plate code\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "app_name = 'regression-house-prices' # you can change this, if you like\n",
    "\n",
    "tb_top_level_dir= '/tmp/tensorboard-logs'\n",
    "tensorboard_logs_dir= os.path.join (tb_top_level_dir, app_name, \n",
    "                                    datetime.datetime.now().strftime(\"%Y-%m-%d--%H-%M-%S\"))\n",
    "print (\"Saving TB logs to : \" , tensorboard_logs_dir)\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=tensorboard_logs_dir, histogram_freq=1)\n",
    "\n",
    "# The patience parameter is the amount of epochs to check for improvement\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10:  Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## TODO start with 100, try 500 and 1000 \n",
    "epochs = 100  ## experiment 100, 500, 1000\n",
    "\n",
    "print (\"training starting ...\")\n",
    "## TODO : to see training output set verbose=2\n",
    "history = model.fit(\n",
    "              x_train, y_train,\n",
    "              epochs=epochs, validation_split = 0.2, verbose=0,\n",
    "              callbacks=[early_stop, tensorboard_callback])\n",
    "\n",
    "print (\"training done.\")\n",
    "\n",
    "##TODO : how long is the training taking?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['mean_squared_error'], label='mse')\n",
    "plt.plot(history.history['mean_absolute_error'], label='mae')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12 : Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = model.metrics_names\n",
    "print (\"model metrics : \" , metric_names)\n",
    "metrics = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "for idx, metric in enumerate(metric_names):\n",
    "    print (\"Metric : {} = {:,.2f}\".format (metric_names[idx], metrics[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)\n",
    "print (x_test)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Evalute prediction output\n",
    "Let's do a pd dataframe and do some plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame(x_test_orig)  # use the original one, not scaled\n",
    "predictions_df['actual_price'] = y_test\n",
    "predictions_df['predicted_price'] = predictions\n",
    "predictions_df['error'] = predictions_df['actual_price'] - predictions_df['predicted_price'] \n",
    "\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "## print sample to see different data every time\n",
    "predictions_df.sample(frac=0.1)\n",
    "## or just print the first few\n",
    "# predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## which house we got really wrong?\n",
    "print (\"biggest error : \")\n",
    "predictions_df.loc[predictions_df['error'].abs().idxmax()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## which house we are spot on?\n",
    "print (\"lowest error\")\n",
    "predictions_df.loc[predictions_df['error'].abs().idxmin()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many house sales, we predicted within 5% ?\n",
    "Let's use 5% margin of error as our benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df['error_percentage'] = predictions_df['error'].abs() * 100 / predictions_df['actual_price']\n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : you can adjust the benchmark target\n",
    "benchmark = 5  # 5%\n",
    "\n",
    "good_predictions = predictions_df[predictions_df['error_percentage'] <= benchmark]\n",
    "\n",
    "good_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meeting_benchmark = good_predictions.shape[0] *100 / predictions_df.shape[0]\n",
    "\n",
    "print (\"number of predictions within benchmark error ({}%) are  =  {:,}  ({:.1f}% of total)\".\n",
    "       format (benchmark, good_predictions.shape[0], meeting_benchmark))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15: Ideas to Try\n",
    "Now that we had done an 'end-to-end' regression implementation, lets tune our algorithm.  \n",
    "\n",
    "**==> Q : What are some fo the things we can do to get a higher performance?**\n",
    "\n",
    "Here are some ideas to get you started\n",
    "- **Idea 1 : Any other inputs we can add?**  \n",
    "  - In Step 5, add a couple more columns as input\n",
    "  - only choose numeric columns at this time\n",
    "  - Try adding 'LandVal'  as an input column.  Run again, did that improve the benchmark performance?\n",
    "  - What would be the implication of adding all the columns?\n",
    "  \n",
    "- **Idea 2 : Remove outliers**  \n",
    "As you noticed, we have quite a bit of outliers (remember the 33 bedroom house? :-).  Outliers tend to skew the results.  So let's remove them\n",
    "  - Step 4 : uncomment the cell.  Here we are filtering only houses that have less than 5 bedrooms\n",
    "  \n",
    "- **Idea 3 : Increase epochs**  \n",
    "  - In Step 10, increase epochs from 100 to 500 to 1000\n",
    "  - Notice the training time will increase\n",
    "  - do you get better results?  why or why not?\n",
    "  \n",
    "- **Idea 4 : Build a Bigger network** \n",
    "  - In Step 8, we are setting up our network.  We are using 64 neurons\n",
    "  - Increase the number of neurons from 64 to 128\n",
    "  - Th\n",
    "  - Does the training time go up?\n",
    "  - Are you getting better accuracy?\n",
    "  - We can also add more layers and build a 'deeper' network.  More on this later\n",
    "  \n",
    "- **Idea 5 : Need more data :-)**  \n",
    "Most of the time, neural networks can yield better results if trained on more data\n",
    "\n",
    "- **Any other ideas?**\n",
    "\n",
    "#### Share your experiments with the class!\n",
    "\n",
    "**What is the best score you have gotten? :-)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Step : Create the most compact code\n",
    "In this notebook we walked you through multiple steps for learning purposes.  \n",
    "Now we are asking you to come up with **bare minimum** code to implement this neural net.  \n",
    "\n",
    "### Class Challenge :-)\n",
    "- Let's see who can come up with most compact code (fewest lines)  \n",
    "- Create a new notebook, and start from scratch\n",
    "- Few hints\n",
    "  - no prints\n",
    "  - minimize comments\n",
    "  - no debug / exploration\n",
    "  \n",
    "**Ready, set, go!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
