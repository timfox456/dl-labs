{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Classification for Prosper Loan Dataset\n",
    "\n",
    "We are going to classify the prosper loan dataset.  This dataset shows a history of loans made by Prosper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print ('tensorflow version:', tf.__version__)\n",
    "print ('devices found:', tf.config.experimental.list_physical_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-GPU Debug\n",
    "The following block tests if TF is running on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This block is to tweak TF running on GPU\n",
    "## You may comment this out, if you are not using GPU\n",
    "\n",
    "\n",
    "import os, sys\n",
    "\n",
    "## disable info logs from TF\n",
    "#   Level | Level for Humans | Level Description                  \n",
    "#  -------|------------------|------------------------------------ \n",
    "#   0     | DEBUG            | [Default] Print all messages       \n",
    "#   1     | INFO             | Filter out INFO messages           \n",
    "#   2     | WARNING          | Filter out INFO & WARNING messages \n",
    "#   3     | ERROR            | Filter out all messages \n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # or any {'0', '1', '2'}\n",
    "tf.get_logger().setLevel('WARN')\n",
    "\n",
    "\n",
    "## ---- start Memory setting ----\n",
    "## Ask TF not to allocate all GPU memory at once.. allocate as needed\n",
    "## Without this the execution will fail with \"failed to initialize algorithm\" error\n",
    "\n",
    "from tensorflow.compat.v1.keras.backend import set_session\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "set_session(sess)\n",
    "## ---- end Memory setting ----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the Data\n",
    "\n",
    "Notice we are first loading this into a Pandas dataframe. This is fine for a small dataset, but we will need more than this for a large \"at scale\" notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "\n",
    "## small file, start with this\n",
    "data_location_local = \"https://s3.amazonaws.com/elephantscale-public/data/prosper-loan/prosper-loan-data-sample.csv\"\n",
    "data_location = \"https://s3.amazonaws.com/elephantscale-public/data/prosper-loan/prosper-loan-data-sample.csv\"\n",
    "\n",
    "## this is a large file\n",
    "# data_location_local = \"../data/prosper-loan/prosper-loan-data.csv.gz\"\n",
    "# data_location = \"https://s3.amazonaws.com/elephantscale-public/data/prosper-loan/prosper-loan-data.csv.gz\"\n",
    "\n",
    "## access the file\n",
    "if not os.path.exists (data_location_local):\n",
    "    data_location_local = keras.utils.get_file(fname=os.path.basename(data_location),\n",
    "                                           origin=data_location)\n",
    "print ('data_location_local:', data_location_local)\n",
    "data = pd.read_csv(data_location_local)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prosper_clean = data.dropna()\n",
    "\n",
    "print(\"Original record count {:,}, cleaned records count {:,},  dropped {:,}\"\\\n",
    "      .format(len(data), len(prosper_clean), \n",
    "              (len(data) - len(prosper_clean))))\n",
    "prosper_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : do you see data skew?\n",
    "## LoanStatus is what we are trying to predict\n",
    "\n",
    "print(prosper_clean['LoanStatus'].value_counts())\n",
    "\n",
    "# 1 - paid\n",
    "# 0 - defaulted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prosper_clean['EmploymentStatus'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prosper_clean['ListingCategory'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Shape Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Select Columns to consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## categorical columns : These columns need to be encoded \n",
    "categorical_columns = ['ListingCategory', 'BorrowerState','EmploymentStatus']\n",
    "label_column = ['LoanStatus']\n",
    "\n",
    "## numeric columns : these columns will be scaled\n",
    "\n",
    "## Approcah 1: We can manually define these columns\n",
    "\n",
    "## TODO : Add 'CreditScore' and 'YearsWithCredit' to this\n",
    "numeric_colums = ['Term', 'BorrowerRate', 'ProsperRating (numeric)', 'ProsperScore', 'EmploymentStatusDuration', \n",
    "                   'CurrentCreditLines', 'OpenCreditLines',  '???', '???'\n",
    "                 ]\n",
    "\n",
    "## Approach 2 : include every thing but categorical and label\n",
    "## TODO Later:  Once you have a base line benchmark, just thrown in all numeric columns and see if it gives better results\n",
    "\n",
    "# numeric_colums = [c for c in prosper_clean.columns if c not in categorical_columns + label_column]\n",
    "\n",
    "input_columns = categorical_columns + numeric_colums\n",
    "\n",
    "print ('categorical columns: ', categorical_columns)\n",
    "print ()\n",
    "print ('numeric columns: ', numeric_colums)\n",
    "print ()\n",
    "print (\"label column : \", label_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"selected data:\")\n",
    "prosper_clean[label_column + input_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Encode Categorical Data\n",
    "\n",
    "**Categorical data** can take on one of a limited, and usually fixed, number of possible values, assigning each individual or other unit of observation to a particular group or nominal category on the basis of some qualitative property. \n",
    "\n",
    "Here we will encode the 3 categorical data columns with **OneHotEncoder** *'ListingCategory', 'BorrowerState', and 'Employmentstatus'*  into numerical data for our model to be able to use this data.  One-hot encoding is a representation of categorical variables as binary vectors.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "class categorical_encoder():\n",
    "    def __init__(self):\n",
    "        self.encoder = OneHotEncoder()\n",
    "    def fit(self, data):\n",
    "        self.encoder.fit(data)\n",
    "        self.columns = data.columns\n",
    "    def transform(self, data):    \n",
    "        columns = list()\n",
    "        for j,col in enumerate(self.columns):\n",
    "            for val in self.encoder.categories_[j]:\n",
    "                columns.append(col + '_is_' + str(val))\n",
    "        return pd.DataFrame(self.encoder.transform(data).toarray(), dtype=bool, columns=columns)\n",
    "    \n",
    "category_encoder = categorical_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_data = prosper_clean[categorical_columns]\n",
    "categorical_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_encoder.fit(categorical_data)\n",
    "categorical_data_encoded = category_encoder.transform(categorical_data)\n",
    "categorical_data_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Numerical Data \n",
    "\n",
    "**Numerical data** is data that is measurable, such as time, height, weight, amount, and so on. You can help yourself identify numerical data by seeing if you can average or order the data in either ascending or descending order.\n",
    "\n",
    "We will normalize the numerical data columns using **StandardScaler.**  StandardScaler, standardize features by removing the mean and scaling to unit variance.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "numerical_data = prosper_clean[numeric_colums]\n",
    "ss = StandardScaler()\n",
    "numerical_data_scaled = pd.DataFrame(ss.fit_transform(numerical_data), columns = numerical_data.columns, dtype='float32')\n",
    "numerical_data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = prosper_clean [input_columns]\n",
    "x = pd.concat([categorical_data_encoded, numerical_data_scaled], axis = 1)\n",
    "y = prosper_clean[label_column]\n",
    "\n",
    "print (y.head())\n",
    "print('-----')\n",
    "x\n",
    "\n",
    "## TODO : Inspect the final data.  Does it look correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Ensure Columns are in correct data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO-Later: Once you get the program working, comment out the folowing and run the notebook\n",
    "##  See what happens.  It is a good error to remember :-) \n",
    "\n",
    "## convert input columns to float\n",
    "for col in x.columns:\n",
    "    x[col] = x[col].astype('float32')\n",
    "    \n",
    "## y column is int64\n",
    "## If running on GPUs convert to int8, save some memory\n",
    "y = data[label_column].astype('int8')\n",
    "\n",
    "print (y.head())\n",
    "print('-----')\n",
    "x\n",
    "\n",
    "## TODO : Inspect the final data.  Does it look correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Create train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## TODO : Split the data 80% / 20%\n",
    "## Hint : test_size = 0.2\n",
    "x_train,x_test, y_train,y_test = train_test_split(x,y,test_size=???,random_state=0) \n",
    "\n",
    "# # backup\n",
    "# x_train_bak = x_train.copy(deep=True)\n",
    "# x_test_bak = x_test.copy(deep=True)\n",
    "\n",
    "print (\"x_train.shape : \", x_train.shape)\n",
    "print (\"y_train.shape : \", y_train.shape)\n",
    "print (\"x_test.shape : \", x_test.shape)\n",
    "print (\"y_test.shape : \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : Build the Model\n",
    "Since this is a classifier, here is how we are going to build the neural network\n",
    "- Neurons in Input layer  = input dimensions (4 here)\n",
    "- Neurons in hidden layer = ???\n",
    "- Neurons in Output layer = output classes (binary)\n",
    "- Output activation is 'sigmoid'\n",
    "\n",
    "**Optimizers** trains models fast, but it also prevents them from getting stuck in a local minimum. Optimizers are the engine of machine learning — they make the computer learn.\n",
    "\n",
    "Here are the **optimizers** we will be working with and you can change:\n",
    "- **RMSprop**, gradient-based optimization technique using a moving average of squared gradients to normalize the gradient itself\n",
    "- **Adam**, is an adaptive learning rate optimization algorithm that's been designed specifically for training deep neural networks. The algorithms leverages the power of adaptive learning rates methods to find individual learning rates for each parameter.\n",
    "\n",
    "### TODO : Sketch the neural net\n",
    "- What is the input dimensions\n",
    "- how many neurons in layers\n",
    "- how many output neurons\n",
    "\n",
    "<img src=\"../assets/images/neural-net-unknown.png\" style=\"width:40%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO build a network\n",
    "##    - number of neurons @ first hidden layer = 64, activation=tf.nn.relu\n",
    "##    - Dropout layer :  Dropout (0.2) - drop 20% of signals  -\n",
    "##    - neurons for second hidden layer :  32,  activation=tf.nn.relu\n",
    "##    - final output layer : 1 neuron, activation=tf.nn.sigmoid\n",
    "\n",
    "## TODO-Later:  Remove Dropout layers, and run it again.\n",
    "##              Does it make a differencen in results?  Can you explain?\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    # input layer is implicit\n",
    "    tf.keras.layers.Dense(units=??, activation=???, input_dim=x.shape[1]),\n",
    "    tf.keras.layers.Dropout(???),\n",
    "    tf.keras.layers.Dense(units=???, activation=???),\n",
    "    tf.keras.layers.Dropout(???),\n",
    "    \n",
    "        ## TODO-Later : Experiment by adding more layers?\n",
    "#     tf.keras.layers.Dense(units=16,activation=tf.nn.relu),\n",
    "#     tf.keras.layers.Dropout(0.2),\n",
    "#     tf.keras.layers.Dense(8,activation=tf.nn.relu),\n",
    "#     tf.keras.layers.Dropout(0.2),\n",
    "\n",
    "    tf.keras.layers.Dense(units=???, activation=???)\n",
    "  ])\n",
    "\n",
    "## include a bunch of metrics\n",
    "metrics = [\n",
    "    'accuracy',\n",
    "    tf.keras.metrics.TruePositives(name='tp'),\n",
    "    tf.keras.metrics.FalsePositives(name='fp'),\n",
    "    tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "    tf.keras.metrics.FalseNegatives(name='fn'),\n",
    "    tf.keras.metrics.Precision(name='precision'),\n",
    "    tf.keras.metrics.Recall(name='recall'),\n",
    "    tf.keras.metrics.AUC(name='auc')\n",
    "  ]\n",
    "\n",
    "# metrics = ['accuracy' ]\n",
    "\n",
    "## TODO-Later: Experiment with different optimizers.\n",
    "##   Do they make a diff?\n",
    "\n",
    "# opt = tf.keras.optimizers.RMSprop()\n",
    "# opt=tf.keras.optimizers.RMSprop(lr=0.000001)\n",
    "opt = 'adam'\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                optimizer=opt,\n",
    "                metrics=metrics)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "tf.keras.utils.plot_model(model, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 : Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is fairly boiler plate code\n",
    "import datetime\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "app_name = 'classification-prosper'\n",
    "# timestamp  = datetime.datetime.now().strftime(\"%Y-%m-%d--%H-%M-%S\")\n",
    "tb_top_level_dir= '/tmp/tensorboard-logs'\n",
    "tb_app_dir = os.path.join (tb_top_level_dir, app_name)\n",
    "tb_logs_dir = os.path.join (tb_app_dir, datetime.datetime.now().strftime(\"%H-%M-%S\"))\n",
    "print (\"Saving TB logs to : \" , tb_logs_dir)\n",
    "#clear out old logs\n",
    "shutil.rmtree ( tb_app_dir, ignore_errors=True )\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_logs_dir, write_graph=True, \n",
    "                                                      write_images=True, histogram_freq=1)\n",
    "\n",
    "## This will embed Tensorboard right here in jupyter!\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir $tb_logs_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 : Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "## TODO : Start with epochs 10.  Observe the training\n",
    "epochs = ???\n",
    "\n",
    "print (\"training starting ...\")\n",
    "## TODO : specify 20% data for validation (Hint : validation_split = 0.2)\n",
    "history = model.fit(\n",
    "              x_train, y_train,\n",
    "              epochs=epochs, validation_split = ???, verbose=1,\n",
    "              callbacks=[tensorboard_callback])\n",
    "print (\"training done.\")\n",
    "\n",
    "## TODO-Later : Try to increase the epochs\n",
    "## TODO-Later : Training taking too long?  Switch to GPU !\n",
    "##              Observe the debug output on the top to make sure you are infact using GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 : Plot History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if 'accuracy' in history.history:\n",
    "    plt.plot(history.history['accuracy'], label='train_accuracy')\n",
    "if 'val_accuracy' in history.history:\n",
    "    plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 : Predict\n",
    "\n",
    "**predict**  will return the scores of the regression\n",
    "   \n",
    "**predict_classes**  will return the class of your prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "## Raw predictions are probabilities\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "## we need to map the probabilities into 0/1\n",
    "y_pred = predictions2 = [0 if n < 0.5 else 1 for n in predictions]\n",
    "\n",
    "np.set_printoptions(formatter={'float': '{: 0.2f}'.format})\n",
    "\n",
    "print ('predictions : ' , predictions[:10])\n",
    "print ('prediction2: ' , predictions2[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 : Evaluate the model\n",
    "\n",
    "### 9.1 - Print out metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = model.metrics_names\n",
    "print (\"model metrics : \" , metric_names)\n",
    "\n",
    "metrics = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "for idx, metric in enumerate(metric_names):\n",
    "    print (\"Metric : {} = {:,.2f}\".format (metric_names[idx], metrics[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 - Confussion Matrix\n",
    "Since this is a classification problem, confusion matrix is very effective way to evaluate our model\n",
    "\n",
    "Visualizing the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plain confusion matrix \n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, labels = [0,1])\n",
    "cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize = (8,5))\n",
    "\n",
    "# colormaps : cmap=\"YlGnBu\" , cmap=\"Greens\", cmap=\"Blues\",  cmap=\"Reds\"\n",
    "sns.heatmap(cm, annot=True, cmap=\"Reds\", fmt='d').plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 - Metrics calculated from Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(classification_report(y_test, y_pred, output_dict=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO : Intepret confusion matrix\n",
    "Instructor will walk you through the matrix.  \n",
    "Answer these questions\n",
    "- which class is classified correctly mostly\n",
    "- which class is classified incorrectly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10 : Improve the Model\n",
    "\n",
    "Inspect the following\n",
    "- What is the metric 'accuracy' in step 9.1\n",
    "- And verify this with tensorboard (port 6066)\n",
    "\n",
    "Most likely, we didn't get a great accuracy.  \n",
    "How can we improve it?\n",
    "\n",
    "**Try the following ideas** \n",
    "\n",
    "- **Idea-1 : Increase neurons in hidden layer**  \n",
    "  - In Step-4, increase hidden layer neurons from 8 --> 64  \n",
    "  - Click 'Kernel --> Restart and Run all Cells'  \n",
    "  - Hopefully you should see improvement in the accuracy.  \n",
    "  - Check  accuracy metrics / confusion matrix / tensorboard\n",
    "- **Idea-2 : Increase epochs**\n",
    "  - Increasing the epochs may cause cause your data to overfit\n",
    "  - Look at time and how long it will take to run when increasing epochs\n",
    "- **Idea-3 : Change optimizers** \n",
    " - The optimizer interacts with the initialization scheme, so this might need to be changed.\n",
    " - The learning rate may need to be changed.\n",
    " - The learning rate schedule may need to be adjusted.\n",
    "- **Idea-3 : Change scalers**\n",
    "  - Try different scalers\n",
    "  - Try data without using a scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup \n",
    "Before running the next exercise, run the following cell to terminate processes and free up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Kill any child processes (like tensorboard)\n",
    "\n",
    "import psutil\n",
    "import os, signal\n",
    "\n",
    "current_process = psutil.Process()\n",
    "children = current_process.children(recursive=True)\n",
    "for child in children:\n",
    "    print('Killing Child pid  {}'.format(child.pid))\n",
    "    os.kill(child.pid, signal.SIGKILL)\n",
    "    \n",
    "## This will kill actual kernel itself\n",
    "# os.kill(os.getpid(), signal.SIGKILL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
